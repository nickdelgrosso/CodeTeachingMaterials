---
title: "Effect Size with Cohen's D and Z-Scaling"
author: "Nicholas A. Del Grosso"
date: "2023-02-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Useful functions for this notebook:
```{r}
library(ggplot2)

plothistogram <- function(data, title=""){
  (ggplot() 
  + geom_histogram(mapping = aes(x=x), data = data.frame(x=data)) 
  + labs(title=title)
  )
}

plot2histograms <- function(data1, data2, title="") {
  (ggplot() 
  + geom_histogram(mapping = aes(x=x), data = data.frame(x=data1), alpha=.5, fill='red') 
  + geom_histogram(mapping = aes(x=x), data = data.frame(x=data2), alpha=.5, fill='blue')
  + labs(title=title)
  )
}

# install.packages("withr")
library(withr)
generate.data = function(n, seed){

}
```


Scenario: The duration (in minutes) of a deep sleep and REM sleep episodes after taking our new drug, 
compared to normal avarage episode lengths.

# How Large was the Effect I Measured?

When reporting an effect size, we often want to report a *difference* between two values.
In the case where we are measuring the mean, this would be:
  - The difference between the mean of our sample and some other value we care about (Often 0)
  - The difference between the mean of one set of samples and the mean of another set of samples.
  
There is a problem with just subtracting the means, though: it doesn't take the *variability* of our
sample into account.  For example, look at the two histograms below. Each shows two histograms with 
means of an equal distance, but in one case there is less variability, and in the other case there is
more.  Which case should we feel like we found a stronger effect?

```{r}
# install.packages("effsize")
library(effsize)
library(patchwork)  # lets you make subplots by joining them with | and / (e.g. p1|p2 makes them side-by-side)


x1 <- rnorm(2000, mean=4, sd=1)
x2 <- rnorm(2000, mean=8, sd=1)
plot1 <- plot2histograms(x1, x2, title="SD: 1")

y1 <- rnorm(2000, mean=4, sd=3)
y2 <- rnorm(2000, mean=8, sd=3)
plot2 <- plot2histograms(y1, y2, title="SD: 4")

plot1 | plot2


```


We'll see in another unit on that both of these studies would consider the difference of the two samples' means
"significant".  And on top of that, the difference in means are the same; if this was data from a study showing
the efffect of taking some drug, then the report from both studies would say, "On average, the drug improved 
things by 4."!  But that's not very representative of most patients in the study; in fact, in the case where the two plots highly overlap, many of the patients would have gotten worse.

So what's a number we can use to say how strong an effect we found, taking into account the variability of the data?
This is the purpose of **effect size** metrics.  There are many of them out there to choose from, and they all have some
shared characteristics:
  1. They calculate a single number that factors in both the model and the error.
  2. That number often comes with a rule of thumb interpretation (i.e. "0.2 is usually a small effect, while 0.8 is a large effect").  This is great for prioritizing, planning, and communicating studies.
  
Some common effect size measurements are:
  1. Cohen's d
  2. *Hedge's g*
  3. Pearson's r
  
  
We'll start out with some simple effect size calculations, 
then move to using the `effsize` library to do these estimations and try them out.


# Comparing one sample to a theoretical mean

```{r}
# Data
reference.value <- 3
x <- rnorm(30, mean=4, sd=1)

```







## Scenario 1:


  

Dataset:
```{r}
deep.sleep.normal <- 15.8
deep.sleep <- c(21.0, 18.6, 24.0, 23.0, 23.8, 19.4, 20.2, 18.8, 21.6, 19.6, 22.0, 18.4, 22.4, 19.8, 24.2, 21.4, 18.2, 19.4, 20.8, 19.4, 19.8, 21.8, 22.4, 19.4, 20.4, 19.4, 22.2, 19.4, 20.6, 19.4)
  )

rem.sleep.normal = 0.3
REM_sleep <- c(3.0, 2.9, 2.9, 2.9, 2.7, 2.9, 2.8, 2.9, 2.7, 3.0, 2.9, 2.9, 2.7, 3.0, 2.8, 2.9, 2.7, 2.9, 2.9, 2.9, 2.7, 2.9, 2.8, 3.0, 2.7, 2.9, 2.9, 2.7, 2.8, 2.9)

paste(mean(deep.sleep), sd(deep.sleep))
mean(deep.sleep) - deep.sleep.normal
paste(mean(REM_sleep), sd(REM_sleep))
mean(REM_sleep) - rem.sleep.normal

(mean(deep.sleep) - deep.sleep.normal) / sd(deep.sleep)
(mean(REM_sleep) - rem.sleep.normal) / sd(rem.sleep)
```


```{r}
x <- c( 7, 5.1, 0.5, 4.6, 1.4, 1, 7.7, 4.5, 0.2, 2.5 )
y <-c( 3.5, 3, 1.4, 2.9, 1.8, 1.7, 3.7, 2.8, 1.6, 2.2 )
# Note: different mean differences, same effect size
```





```{r}

# Note: Opposite mean differences and effect size
set.seed(45)
x<-abs(round(rnorm(20, 10, 7), 1))
cat("c(", paste0(x, collapse = ", "), ")\n")
mean(x) - 2
(mean(x) - 2) / sd(x)
cat('\n')

y<-abs(round(rnorm(20, 5, 1), 1))
cat("c(", paste0(y, collapse = ", "), ")\n")
mean(y) - 2
(mean(y) - 2) / sd(y)
```


```{r}

# Note: Different Mean differences, similar variability
set.seed(45)
x<-abs(round(rnorm(20, 10, 4), 1))
cat("c(", paste0(x, collapse = ", "), ")\n")
mean(x) - 2
(mean(x) - 2) / sd(x)
cat('\n')

y<-abs(round(rnorm(20, 5, 4), 1))
cat("c(", paste0(y, collapse = ", "), ")\n")
mean(y) - 2
(mean(y) - 2) / sd(y)
```


```{r}

# Note: Similar Mean differences, different variability
set.seed(45)
x<-abs(round(rnorm(300, 20, 3), 1))
# cat("c(", paste0(x, collapse = ", "), ")\n")
mean(x) - 15
(mean(x) - 2) / sd(x)
cat('\n')

y<-abs(round(rnorm(300, 10, 3), 1))
# cat("c(", paste0(y, collapse = ", "), ")\n")
mean(y) - 5
(mean(y) - 2) / sd(y)
```






